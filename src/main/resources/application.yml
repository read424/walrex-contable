quarkus:
  application:
    name: admon-catalogo
  http:
    body:
      uploads-directory: tmp/uploads
      merge-form-attributes: true
    limits:
      max-body-size: 10M
    port: 8089
    non-application-root-path: /q
  devservices:
    enabled: false
  test:
    integration-test-profile: test
  datasource:
    devservices:
      enabled: false
    db-kind: postgresql
    username: postgres
    password: 12345
    reactive:
      url: postgresql://127.0.0.1:5432/walrex_db
      max-size: 20
  swagger-ui:
    always-include: true
    path: /swagger-ui
  redis:
    devservices:
      enabled: false
    hosts: redis://127.0.0.1:6379
    timeout: 10s
    max-pool-size: 20
    max-pool-waiting: 24
  cache:
    redis:
      expire-after-write: 30m
      prefix: "wx-admon"
  otel:
    enabled: true
    sdk:
      disabled: false
    service:
      name: admon-catalogo
    traces:
      sampler:
        arg: 1.0
    exporter:
      otlp:
        endpoint: http://localhost:4317
    propagators:
      - tracecontext
      - baggage
    resource:
      attributes:
        service.name: admon-catalogo
        service.version: 1.0.0-SNAPSHOT
        deployment.environment: dev
  micrometer:
    enabled: true
    registry-enabled-default: false
    export:
      prometheus:
        path: /q/metrics
    binder-enabled-default: true
    binder:
      jvm: true
      system: true
      http-server:
        enabled: true
        max-uri-tags: 100
      http-client:
        enabled: true
      vertx:
        enabled: true
      mp-metrics:
        enabled: true
  log:
    level: INFO
    min-level: TRACE
  smallrye-fault-tolerance:

  langchain4j:
    devservices:
      enabled: false

    # Define default providers to resolve ambiguity for any unqualified injections
    chat-model:
      provider: openai
    embedding-model:
      provider: huggingface

    # Named model provider assignments
    groq-chat:
      chat-model:
        provider: openai
    ollama-chat:
      chat-model:
        provider: ollama
    ollama-embed:
      embedding-model:
        provider: ollama

    # --- Provider-Centric Configurations ---

    # OpenAI Provider Configuration
    openai:
      # Default OpenAI chat model configuration
      base-url: https://api.groq.com/openai/v1
      api-key: ${GROQ_API_KEY:changeme}
      timeout: 60s
      log-requests: false
      log-responses: false
      chat-model:
        model-name: llama-3.1-8b-instant
        temperature: 0.7
      # Named model for Groq using OpenAI provider, which can be injected with @ModelName("groq-chat")
      groq-chat:
        api-key: ${GROQ_API_KEY:changeme}
        base-url: https://api.groq.com/openai/v1
        timeout: 60s
        log-requests: false
        log-responses: false
        chat-model:
          model-name: llama-3.1-8b-instant
          temperature: 0.7

    # HuggingFace Provider Configuration
    huggingface:
      api-key: ${HUGGING_FACE_API_KEY:}
      embedding-model:
        model-id: "BAAI/bge-large-en-v1.5"
        timeout: 60s

    # Ollama Provider Configuration
    ollama:
      base-url: http://localhost:11434
      timeout: 60s
      # Named model for Ollama chat, to be injected with @ModelName("ollama-chat")
      ollama-chat:
        base-url: http://localhost:11434
        timeout: 60s
        log-requests: false
        log-responses: false
        chat-model:
          model-id: llama3.2
          temperature: 0.7
      # Named model for Ollama embeddings, to be injected with @ModelName("ollama-embed")
      ollama-embed:
        base-url: http://localhost:11434
        timeout: 60s
        embedding-model:
          model-id: mxbai-embed-large

    # Qdrant configuration
    qdrant:
      host: localhost
      port: 6334
      api-key: ${QDRANT_API_KEY:}
      use-tls: false
      collection:
        name: accounting_data
      
  rest-client:
    binance-p2p:
      url: https://p2p.binance.com
      scope: jakarta.enterprise.context.ApplicationScoped
      read-timeout: 30000
      connect-timeout: 10000
    azure-doc-intel:
      url: https://mi-extractor-documentos.cognitiveservices.azure.com
      scope: jakarta.enterprise.context.ApplicationScoped
      read-timeout: 60000
      connect-timeout: 10000
      logging:
        scope: request-response
        body-limit: 1024

azure:
  doc:
    intel:
      key: ${AZURE_DOC_INTEL_KEY:your-azure-key-here}

embeddings:
  sync:
    enabled: true
    on-startup: true
    batch-size: 50

rag:
  llm:
    default-provider: groq
    enable-fallback: true
  search:
    account-limit: 5
    historical-entry-limit: 3
    similarity-threshold: 0.7
  historical:
    auto-sync-enabled: true
    sync-on-create: true
    sync-on-update: false